{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029fa19a",
   "metadata": {},
   "source": [
    "1. 라이브러리 및 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce13ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71abe9",
   "metadata": {},
   "source": [
    "2. JSON 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5a5f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- board_state_before (Top 10) ---\n",
      "[0] [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "[1] [[0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "[2] [[0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[3] [[0, 1, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[4] [[0, 1, 0, 1], [-1, 0, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[5] [[1, 1, 0, 1], [-1, 0, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[6] [[1, 1, 0, 1], [-1, -1, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[7] [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "[8] [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1]]\n",
      "[9] [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, -1, 1]]\n",
      "\n",
      "--- board_state_current (Top 10) ---\n",
      "[0] [[0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "[1] [[0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[2] [[0, 1, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[3] [[0, 1, 0, 1], [-1, 0, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[4] [[1, 1, 0, 1], [-1, 0, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[5] [[1, 1, 0, 1], [-1, -1, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[6] [[1, 1, 1, 1], [-1, -1, 0, 0], [0, 0, 0, 0], [-1, 0, 0, 0]]\n",
      "[7] [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1]]\n",
      "[8] [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, -1, 1]]\n",
      "[9] [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 0], [0, 0, -1, 1]]\n",
      "\n",
      "--- action (Top 10) ---\n",
      "[0] 3\n",
      "[1] 12\n",
      "[2] 1\n",
      "[3] 4\n",
      "[4] 0\n",
      "[5] 5\n",
      "[6] 2\n",
      "[7] 15\n",
      "[8] 14\n",
      "[9] 10\n",
      "\n",
      "--- player (Top 10) ---\n",
      "[0] 1\n",
      "[1] -1\n",
      "[2] 1\n",
      "[3] -1\n",
      "[4] 1\n",
      "[5] -1\n",
      "[6] 1\n",
      "[7] 1\n",
      "[8] -1\n",
      "[9] 1\n",
      "\n",
      "--- winner (Top 10) ---\n",
      "[0] 1\n",
      "[1] 1\n",
      "[2] 1\n",
      "[3] 1\n",
      "[4] 1\n",
      "[5] 1\n",
      "[6] 1\n",
      "[7] 1\n",
      "[8] 1\n",
      "[9] 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "json_path = r\"C:\\Users\\juyeo\\Desktop\\틱택토_학습데이터\\학습데이터\\tictactoe_data_complete.json\"\n",
    "\n",
    "# JSON 데이터 불러오기\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 내부 데이터 추출\n",
    "game_data = data[\"game_data\"]\n",
    "\n",
    "# 특정 컬럼 10개씩 출력하는 함수\n",
    "def print_column_samples(field_name, num_samples=10):\n",
    "    print(f\"\\n--- {field_name} (Top {num_samples}) ---\")\n",
    "    for i in range(min(num_samples, len(game_data))):\n",
    "        print(f\"[{i}] {game_data[i][field_name]}\")\n",
    "\n",
    "# 출력\n",
    "print_column_samples(\"board_state_before\")\n",
    "print_column_samples(\"board_state_current\")\n",
    "print_column_samples(\"action\")\n",
    "print_column_samples(\"player\")\n",
    "print_column_samples(\"winner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d5c3b7",
   "metadata": {},
   "source": [
    "3. 학습 데이터셋 x_train, y_train 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18017489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (12506, 4, 4, 1)\n",
      "y_train.shape: (12506, 16)\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for item in game_data:\n",
    "    board_state_before = item[\"board_state_before\"]  # 길이 16짜리 1D 배열\n",
    "    action = item[\"action\"]  # 0~15 정수\n",
    "\n",
    "    # 입력 데이터: (4,4,1)\n",
    "    board_state_2d = np.array(board_state_before).reshape(4,4,1)\n",
    "    x_train.append(board_state_2d)\n",
    "\n",
    "    # 출력 데이터: (16,) one-hot 인코딩\n",
    "    action_one_hot = np.zeros(16)\n",
    "    action_one_hot[action] = 1\n",
    "    y_train.append(action_one_hot)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f\"x_train.shape: {x_train.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f067ff79",
   "metadata": {},
   "source": [
    "4. Tic Tac Toe Environment 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69e39b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self):\n",
    "    # 보드는 0으로 초기화된 16개의 배열로 준비함\n",
    "    # 게임종료 : done = True\n",
    "        self.board_a = np.zeros(16)\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.winner = 0\n",
    "        self.print = False\n",
    "\n",
    "    def move(self, p1, p2, player):\n",
    "    # 각 플레이어가 선택한 행동을 표시 하고 게임 상태(진행 또는 종료)를 판단\n",
    "    # p1 = 1, p2 = -1로 정의\n",
    "    # 각 플레이어는 행동을 선택하는 select_action 메서드를 가짐\n",
    "        if player == 1:\n",
    "            pos = p1.select_action(self, player)\n",
    "        else:\n",
    "            pos = p2.select_action(self, player)\n",
    "            \n",
    "            \n",
    "        # 보드에 플레이어의 선택을 표시\n",
    "        self.board_a[pos] = player\n",
    "        if self.print:\n",
    "            print(player)\n",
    "            self.print_board()\n",
    "        # 게임이 종료상태인지 아닌지를 판단\n",
    "        self.end_check(player)\n",
    "        \n",
    "        return self.reward, self.done\n",
    "    \n",
    "    # 현재 보드 상태에서 가능한 행동(둘 수 있는 장소)을 탐색하고 리스트로 반환\n",
    "    def get_action(self):\n",
    "        observation = []\n",
    "        \n",
    "        for i in range(16):\n",
    "            if self.board_a[i] == 0:\n",
    "                observation.append(i)\n",
    "        return observation\n",
    "    \n",
    "    # 게임이 종료(승패 또는 비김)됐는지 판단\n",
    "    def end_check(self, player):\n",
    "        # 0   1   2   3\n",
    "        # 4   5   6   7\n",
    "        # 8   9  10  11\n",
    "        #12  13  14  15\n",
    "        \n",
    "        # 4 x 4\n",
    "        end_condition = [  \n",
    "            # 가로\n",
    "            (0,1,2,3), (4,5,6,7), (8,9,10,11), (12,13,14,15),\n",
    "            # 세로\n",
    "            (0,4,8,12), (1,5,9,13), (2,6,10,14), (3,7,11,15),\n",
    "            # 대각선\n",
    "            (0,5,10,15), (3,6,9,12)\n",
    "        ]\n",
    "        for line in end_condition:\n",
    "             if self.board_a[line[0]] == self.board_a[line[1]] \\\n",
    "                and self.board_a[line[1]] == self.board_a[line[2]] \\\n",
    "                and self.board_a[line[2]] == self.board_a[line[3]] \\\n",
    "                and self.board_a[line[0]] != 0:  \n",
    "                # 종료됐다면 누가 이겼는지 표시\n",
    "                self.done = True\n",
    "                self.reward = player\n",
    "                return\n",
    "        # 비긴 상태는 더는 보드에 빈 공간이 없을때\n",
    "        observation = self.get_action()\n",
    "        if (len(observation)) == 0:\n",
    "            self.done = True\n",
    "            self.reward = 0            \n",
    "        return\n",
    "\n",
    "    # 현재 보드의 상태를 표시 p1 = O, p2 = X    \n",
    "    def print_board(self): #4x4로 수정 완료\n",
    "        print(\"+----+----+----+----+\")  \n",
    "        for i in range(4): # 줄 수 \n",
    "            for j in range(4):  # 칸 수 \n",
    "                if self.board_a[4*i+j] == 1:  #index 계산식도 4*i+j로 바꿈\n",
    "                    print(\"|  O\",end=\" \")\n",
    "                elif self.board_a[4*i+j] == -1:\n",
    "                    print(\"|  X\",end=\" \")\n",
    "                else:\n",
    "                    print(\"|   \",end=\" \")\n",
    "            print(\"|\")\n",
    "            print(\"+----+----+----+----+\")  # 출력 테두리 4칸용으로 바꿈 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0bf1c",
   "metadata": {},
   "source": [
    "5. human player 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc66635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human_player():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"Human player\"\n",
    "        \n",
    "    def select_action(self, env, player):\n",
    "        while True:\n",
    "            # 가능한 행동을 조사한 후 표시\n",
    "            available_action = env.get_action()\n",
    "            print(\"possible actions = {}\".format(available_action))\n",
    "\n",
    "            # 4x4 상태 번호 표시로 수정 완료\n",
    "            print(\"+----+----+----+----+\")  \n",
    "            print(\"+  0 +  1 +  2 +  3 +\") \n",
    "            print(\"+----+----+----+----+\") \n",
    "            print(\"+  4 +  5 +  6 +  7 +\")  \n",
    "            print(\"+----+----+----+----+\")  \n",
    "            print(\"+  8 +  9 + 10 + 11 +\")  \n",
    "            print(\"+----+----+----+----+\") \n",
    "            print(\"+ 12 + 13 + 14 + 15 +\")  \n",
    "            print(\"+----+----+----+----+\")  \n",
    "    \n",
    "            # 키보드로 가능한 행동을 입력 받음\n",
    "            action = input(\"Select action(human) : \")\n",
    "            action = int(action)\n",
    "            \n",
    "            # 입력받은 행동이 가능한 행동이면 반복문을 탈출\n",
    "            if action in available_action:\n",
    "                return action\n",
    "            # 아니면 행동 입력을 반복\n",
    "            else:\n",
    "                print(\"You selected wrong action\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56726683",
   "metadata": {},
   "source": [
    "6. Random player 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ce1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_player():\n",
    "    def __init__(self):\n",
    "        self.name = \"Random player\"\n",
    "\n",
    "    def select_action(self, env, player):\n",
    "        available_actions = env.get_action()\n",
    "        return np.random.choice(available_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd9039",
   "metadata": {},
   "source": [
    "7. CNN player 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14f9bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_player():\n",
    "    def __init__(self):\n",
    "        self.name = \"CNN_player\"\n",
    "        self.model = self.build_model()\n",
    "        self.print = False\n",
    "\n",
    "    def build_model(self):\n",
    "        \n",
    "        # CNN 모델 생성\n",
    "        model = Sequential()\n",
    "        \n",
    "        # 합성곱층: 4x4 입력, 채널=1, 필터=32, relu\n",
    "        model.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(4,4,1)))\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # 완전연결층\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(16, activation='linear')) # 16개의 출력: 각 칸의 점수\n",
    "        model.compile(optimizer=SGD(lr=0.01), loss='mean_squared_error', metrics=['mse'])\n",
    "        #원래 학습률 : 0.01\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def state_convert(self, board_a):\n",
    "        \n",
    "         # 1D 배열(16칸)을 4x4x1 배열로 변환\n",
    "        state = np.zeros((4,4,1))\n",
    "        \n",
    "        for i in range(16):\n",
    "            state[i//4, i%4, 0] = board_a[i]\n",
    "            \n",
    "        return np.expand_dims(state, axis=0)\n",
    "\n",
    "    def select_action(self, env, player):\n",
    "        \n",
    "        # CNN 모델로 Q-value 예측 → argmax 선택\n",
    "        state = self.state_convert(env.board_a)\n",
    "        q_values = self.model.predict(state, verbose=0)[0]\n",
    "        available_actions = env.get_action()\n",
    "        available_q_values = q_values[available_actions]\n",
    "        selected_index = np.argmax(available_q_values)\n",
    "        action = available_actions[selected_index]\n",
    "        \n",
    "        if self.print:\n",
    "            print(f\"Q-values: {np.round(q_values,2)}\")\n",
    "            print(f\"Available actions: {available_actions}\")\n",
    "            print(f\"Selected action: {action}\")\n",
    "        return action\n",
    "\n",
    "    def train(self, x_train, y_train, epochs=20, verbose=1):\n",
    "        self.model.fit(x_train, y_train, epochs=epochs, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a2066",
   "metadata": {},
   "source": [
    "8. CNN 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d7cf6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "CNN 모델 학습 시작...\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\juyeo\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "12506/12506 [==============================] - 1s 85us/step - loss: 0.0657 - mean_squared_error: 0.0657\n",
      "Epoch 2/30\n",
      "12506/12506 [==============================] - 1s 65us/step - loss: 0.0620 - mean_squared_error: 0.0620\n",
      "Epoch 3/30\n",
      "12506/12506 [==============================] - 1s 65us/step - loss: 0.0609 - mean_squared_error: 0.0609\n",
      "Epoch 4/30\n",
      "12506/12506 [==============================] - 1s 85us/step - loss: 0.0604 - mean_squared_error: 0.0604\n",
      "Epoch 5/30\n",
      "12506/12506 [==============================] - 1s 89us/step - loss: 0.0600 - mean_squared_error: 0.0600\n",
      "Epoch 6/30\n",
      "12506/12506 [==============================] - 1s 91us/step - loss: 0.0598 - mean_squared_error: 0.0598\n",
      "Epoch 7/30\n",
      "12506/12506 [==============================] - 1s 87us/step - loss: 0.0596 - mean_squared_error: 0.0596\n",
      "Epoch 8/30\n",
      "12506/12506 [==============================] - 1s 88us/step - loss: 0.0595 - mean_squared_error: 0.0595\n",
      "Epoch 9/30\n",
      "12506/12506 [==============================] - 1s 89us/step - loss: 0.0593 - mean_squared_error: 0.0593\n",
      "Epoch 10/30\n",
      "12506/12506 [==============================] - 1s 90us/step - loss: 0.0592 - mean_squared_error: 0.0592\n",
      "Epoch 11/30\n",
      "12506/12506 [==============================] - 1s 82us/step - loss: 0.0591 - mean_squared_error: 0.0591\n",
      "Epoch 12/30\n",
      "12506/12506 [==============================] - 1s 85us/step - loss: 0.0590 - mean_squared_error: 0.0590\n",
      "Epoch 13/30\n",
      "12506/12506 [==============================] - 1s 85us/step - loss: 0.0590 - mean_squared_error: 0.0590\n",
      "Epoch 14/30\n",
      "12506/12506 [==============================] - 1s 98us/step - loss: 0.0589 - mean_squared_error: 0.0589\n",
      "Epoch 15/30\n",
      "12506/12506 [==============================] - 1s 82us/step - loss: 0.0588 - mean_squared_error: 0.0588\n",
      "Epoch 16/30\n",
      "12506/12506 [==============================] - 1s 89us/step - loss: 0.0588 - mean_squared_error: 0.0588\n",
      "Epoch 17/30\n",
      "12506/12506 [==============================] - 1s 88us/step - loss: 0.0587 - mean_squared_error: 0.0587\n",
      "Epoch 18/30\n",
      "12506/12506 [==============================] - 1s 88us/step - loss: 0.0587 - mean_squared_error: 0.0587\n",
      "Epoch 19/30\n",
      "12506/12506 [==============================] - 1s 92us/step - loss: 0.0586 - mean_squared_error: 0.0586\n",
      "Epoch 20/30\n",
      "12506/12506 [==============================] - 1s 87us/step - loss: 0.0586 - mean_squared_error: 0.0586\n",
      "Epoch 21/30\n",
      "12506/12506 [==============================] - 1s 87us/step - loss: 0.0586 - mean_squared_error: 0.0586\n",
      "Epoch 22/30\n",
      "12506/12506 [==============================] - 1s 89us/step - loss: 0.0585 - mean_squared_error: 0.0585\n",
      "Epoch 23/30\n",
      "12506/12506 [==============================] - 1s 85us/step - loss: 0.0585 - mean_squared_error: 0.0585\n",
      "Epoch 24/30\n",
      "12506/12506 [==============================] - 1s 89us/step - loss: 0.0585 - mean_squared_error: 0.0585\n",
      "Epoch 25/30\n",
      "12506/12506 [==============================] - 1s 84us/step - loss: 0.0585 - mean_squared_error: 0.0585\n",
      "Epoch 26/30\n",
      "12506/12506 [==============================] - 1s 86us/step - loss: 0.0584 - mean_squared_error: 0.0584\n",
      "Epoch 27/30\n",
      "12506/12506 [==============================] - 1s 89us/step - loss: 0.0584 - mean_squared_error: 0.0584\n",
      "Epoch 28/30\n",
      "12506/12506 [==============================] - 1s 87us/step - loss: 0.0584 - mean_squared_error: 0.0584\n",
      "Epoch 29/30\n",
      "12506/12506 [==============================] - 1s 87us/step - loss: 0.0584 - mean_squared_error: 0.0584\n",
      "Epoch 30/30\n",
      "12506/12506 [==============================] - 1s 88us/step - loss: 0.0583 - mean_squared_error: 0.0583\n",
      "학습 완료!\n"
     ]
    }
   ],
   "source": [
    "cnn_player = CNN_player()\n",
    "\n",
    "print(\"CNN 모델 학습 시작...\")\n",
    "cnn_player.train(x_train, y_train, epochs=30, verbose=1) #원래 에폭 30\n",
    "print(\"학습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb62f9a",
   "metadata": {},
   "source": [
    "9. 게임 실행 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77eca7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl player : Human player\n",
      "p2 player : CNN_player\n",
      "possible actions = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n",
      "Select action(human) : 15\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |  O |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|    |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |  O |\n",
      "+----+----+----+----+\n",
      "possible actions = [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n",
      "Select action(human) : 14\n",
      "+----+----+----+----+\n",
      "|    |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |  O |  O |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|    |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |    |  O |  O |\n",
      "+----+----+----+----+\n",
      "possible actions = [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 13]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n",
      "Select action(human) : 13\n",
      "+----+----+----+----+\n",
      "|    |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|    |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|    |  X |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "possible actions = [0, 1, 2, 4, 6, 7, 8, 9, 10, 11]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n",
      "Select action(human) : 11\n",
      "+----+----+----+----+\n",
      "|    |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|    |  X |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |  O |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|    |  X |    |    |\n",
      "+----+----+----+----+\n",
      "|    |    |    |  O |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "possible actions = [1, 2, 4, 6, 7, 8, 9, 10]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n",
      "Select action(human) : 12\n",
      "You selected wrong action\n",
      "possible actions = [1, 2, 4, 6, 7, 8, 9, 10]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n",
      "Select action(human) : 9\n",
      "+----+----+----+----+\n",
      "|  X |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|    |  X |    |    |\n",
      "+----+----+----+----+\n",
      "|    |  O |    |  O |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|    |  X |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  O |    |  O |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "possible actions = [1, 2, 4, 6, 7, 10]\n",
      "+----+----+----+----+\n",
      "+  0 +  1 +  2 +  3 +\n",
      "+----+----+----+----+\n",
      "+  4 +  5 +  6 +  7 +\n",
      "+----+----+----+----+\n",
      "+  8 +  9 + 10 + 11 +\n",
      "+----+----+----+----+\n",
      "+ 12 + 13 + 14 + 15 +\n",
      "+----+----+----+----+\n",
      "Select action(human) : 10\n",
      "+----+----+----+----+\n",
      "|  X |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|    |  X |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "+----+----+----+----+\n",
      "|  X |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|  X |  X |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "winner is p2(CNN_player)\n",
      "final result\n",
      "+----+----+----+----+\n",
      "|  X |    |    |  X |\n",
      "+----+----+----+----+\n",
      "|  X |  X |    |    |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "|  X |  O |  O |  O |\n",
      "+----+----+----+----+\n",
      "More Game? (y/n)n\n",
      "p1(Human player) = 0 p2(CNN_player) = 1 draw = 0\n"
     ]
    }
   ],
   "source": [
    "## np.random.seed(0)\n",
    "\n",
    "# p1 = cnn_player\n",
    "# p2 = Human_player()\n",
    "\n",
    "p1 = Human_player()\n",
    "p2 = cnn_player\n",
    "\n",
    "# p1 = trained_p1\n",
    "# p2 = trained_p2\n",
    "# p2 = Human_player()\n",
    "\n",
    "# 지정된 게임 수를 자동으로 두게 할 것인지 한게임씩 두게 할 것인지 결정\n",
    "# auto = True : 지정된 판수(games)를 자동으로 진행 \n",
    "# auto = False : 한판씩 진행\n",
    "\n",
    "auto = False\n",
    "\n",
    "# auto 모드의 게임수\n",
    "games = 100\n",
    "\n",
    "print(\"pl player : {}\".format(p1.name))\n",
    "print(\"p2 player : {}\".format(p2.name))\n",
    "\n",
    "# 각 플레이어의 승리 횟수를 저장\n",
    "p1_score = 0\n",
    "p2_score = 0\n",
    "draw_score = 0\n",
    "\n",
    "\n",
    "if auto: \n",
    "    # 자동 모드 실행\n",
    "    for j in tqdm(range(games)):\n",
    "        \n",
    "        np.random.seed(j)\n",
    "        env = Environment()\n",
    "        \n",
    "        for i in range(10000):\n",
    "            # p1 과 p2가 번갈아 가면서 게임을 진행\n",
    "            # p1(1) -> p2(-1) -> p1(1) -> p2(-1) ...\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            # 게임 종료 체크\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    draw_score += 1\n",
    "                break\n",
    "\n",
    "else:                \n",
    "    # 한 게임씩 진행하는 수동 모드\n",
    "    np.random.seed(1)\n",
    "    while True:\n",
    "        \n",
    "        env = Environment()\n",
    "        env.print = False\n",
    "        for i in range(10000):\n",
    "            reward, done = env.move(p1,p2,(-1)**i)\n",
    "            env.print_board()\n",
    "            if done == True:\n",
    "                if reward == 1:\n",
    "                    print(\"winner is p1({})\".format(p1.name))\n",
    "                    p1_score += 1\n",
    "                elif reward == -1:\n",
    "                    print(\"winner is p2({})\".format(p2.name))\n",
    "                    p2_score += 1\n",
    "                else:\n",
    "                    print(\"draw\")\n",
    "                    draw_score += 1\n",
    "                break\n",
    "        \n",
    "        # 최종 결과 출력        \n",
    "        print(\"final result\")\n",
    "        env.print_board()\n",
    "\n",
    "        # 한게임 더?최종 결과 출력 \n",
    "        answer = input(\"More Game? (y/n)\")\n",
    "\n",
    "        if answer == 'n':\n",
    "            break           \n",
    "\n",
    "print(\"p1({}) = {} p2({}) = {} draw = {}\".format(p1.name, p1_score,p2.name, p2_score,draw_score))\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
